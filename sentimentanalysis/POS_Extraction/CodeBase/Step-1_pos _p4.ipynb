{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import ast\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "seperator = '#;'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bingLui = [bingline.rstrip(\"\\n\") for bingline in open(\"../../Bing Liu/Hu and Liu Sentiment Lexicon/positive-words.txt\", encoding='utf8')]\n",
    "bingLuiArray = []\n",
    "for row in bingLui:\n",
    "    bingLuiArray.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = [line.rstrip(\"\\n\") for line in open(\"../TrainingData/trainingData.txt\", encoding='utf8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordtokanize(sentence):\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    word_tokens = [token.lower() for token in word_tokens]\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def countOccurance(sequence_of_sentences):\n",
    "    counts = Counter()\n",
    "    for word in sequence_of_sentences:\n",
    "        counts[word] += 1\n",
    "    return dict(counts)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adjusent_Bygram (array,token):\n",
    "    globalArray =[]    \n",
    "    length = 0    \n",
    "    stringlength = len(array)\n",
    "    tokenArray = []\n",
    "    for i in array:    \n",
    "        if(i == token):\n",
    "            if( length ==0):                \n",
    "                info = {array[length],array[length+1]}\n",
    "                tokenArray.append(info)\n",
    "            if( length == stringlength-1):                 \n",
    "                info = {array[length-1],array[length]}\n",
    "                tokenArray.append(info)\n",
    "            if ((length !=0) & (length != stringlength-1)):\n",
    "                info = {array[length],array[length-1]}\n",
    "                info1 = {array[length],array[length+1]}\n",
    "                tokenArray.append(info)\n",
    "                tokenArray.append(info1)                \n",
    "        length = length+1;   \n",
    "    return tokenArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateAdjuacentByGram(wordToken,token):\n",
    "    globalArray =[]\n",
    "    tokenLength = len(token)\n",
    "    if(tokenLength>0):\n",
    "        for i in token:            \n",
    "            a1 = adjusent_Bygram (wordToken,i)\n",
    "            globalArray.append(a1)    \n",
    "    return globalArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"../TrainigDumpFile/wordtokanize1.txt\",\"w\", encoding='utf8') \n",
    "tempPositiveData = []\n",
    "for row in lines:\n",
    "    liuPositiveLexiconWords =[]\n",
    "    a = np.array(row.split(\"\\t\"))  \n",
    "    trainingLable = a[0]\n",
    "    sentence = a[1]    \n",
    "    sentence = sentence.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    word_tokens = tokenizer.tokenize(sentence)\n",
    "    a3 = [w for w in word_tokens if not w in stop_words]\n",
    "    tempLiuPositiveLexiconWords =[]\n",
    "    for v,k in nltk.pos_tag(a3):\n",
    "        for indBing in bingLuiArray:\n",
    "            if(v == indBing):\n",
    "                tempLiuPositiveLexiconWords.append(v)                \n",
    "                break;\n",
    "    occouredPositiveLexicon = list(set(tempLiuPositiveLexiconWords))    \n",
    "    posTagForPositiveOccouredLexicon = nltk.pos_tag(occouredPositiveLexicon)\n",
    "    bygram = generateAdjuacentByGram(a3,occouredPositiveLexicon)\n",
    "    \n",
    "    posLexicon = countOccurance(tempLiuPositiveLexiconWords)   \n",
    "    if(len(posLexicon) > 0):\n",
    "        liuPositiveLexiconWords.append(posLexicon)\n",
    "    tempPositiveData.append(countOccurance(tempLiuPositiveLexiconWords))\n",
    "    f.writelines(str(sentence))\n",
    "    f.writelines(str(seperator))\n",
    "    f.writelines(str(liuPositiveLexiconWords))\n",
    "    f.writelines(str(seperator))\n",
    "    f.writelines(str(bygram))\n",
    "    f.writelines(str(seperator))\n",
    "    f.writelines(str(posTagForPositiveOccouredLexicon))\n",
    "    f.writelines(str(seperator))\n",
    "    f.writelines(str(trainingLable))\n",
    "    f.writelines('\\n')\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "d = defaultdict(int)\n",
    "for i in tempPositiveData:\n",
    "    for k,v in i.items():\n",
    "        d[k] += v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tempTokanizedData = [tempTokanizedData.rstrip(\"\\n\") for tempTokanizedData in open(\"../TrainigDumpFile/wordtokanize1.txt\", encoding='utf8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for indrow in tempTokanizedData:\n",
    "    sentence = indrow.split(';')[0]\n",
    "    positiveWordCount = indrow.split(';')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bingLiuNegativeWord = [indbingline.rstrip(\"\\n\") for indbingline in open(\"../../Bing Liu/Hu and Liu Sentiment Lexicon/negative-words.txt\")]\n",
    "bingLiuNegativeArray = []\n",
    "for row in bingLiuNegativeWord:\n",
    "    bingLiuNegativeArray.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"../TrainigDumpFile/wordtokanize.txt\",\"w\", encoding='utf8') \n",
    "for indrow in tempTokanizedData:\n",
    "    liuNegativeLexiconWords =[]\n",
    "    sentence = indrow.split(seperator)[0]\n",
    "    positiveWordCount = indrow.split(seperator)[1]\n",
    "    positiveBigram = indrow.split(seperator)[2]\n",
    "    positivePosTag = indrow.split(seperator)[3]\n",
    "    trainingLable = indrow.split(seperator)[4]\n",
    "    tempLiuNegativeLexiconWords =[]   \n",
    "    sentence = sentence.lower()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    word_tokens = tokenizer.tokenize(sentence)\n",
    "    a3 = [w for w in word_tokens if not w in stop_words]   \n",
    "    for v,k in nltk.pos_tag(a3):\n",
    "        for indBing in bingLiuNegativeArray:\n",
    "            if(v == indBing):\n",
    "                tempLiuNegativeLexiconWords.append(v)\n",
    "                break;\n",
    "    occouredNegativeLexicon = list(set(tempLiuNegativeLexiconWords))    \n",
    "    negativePosTag = nltk.pos_tag(occouredNegativeLexicon)\n",
    "    \n",
    "    negLexicon = countOccurance(tempLiuNegativeLexiconWords)\n",
    "    \n",
    "    if(len(negLexicon) > 0):\n",
    "        liuNegativeLexiconWords.append(negLexicon)\n",
    "    negativeBigram = generateAdjuacentByGram(a3,occouredNegativeLexicon)\n",
    "    positiveWordArray = []\n",
    "    \n",
    "    f.writelines(sentence)\n",
    "    f.writelines(str(seperator))\n",
    "    f.writelines(positiveWordCount)\n",
    "    f.writelines(str(seperator))\n",
    "    f.writelines(str(liuNegativeLexiconWords))\n",
    "    f.writelines(str(seperator))\n",
    "    f.writelines(str(positiveBigram))\n",
    "    f.writelines(str(seperator))\n",
    "    f.writelines(str(negativeBigram))\n",
    "    f.writelines(str(seperator))\n",
    "    f.writelines(str(positivePosTag))\n",
    "    f.writelines(str(seperator))\n",
    "    f.writelines(str(negativePosTag))\n",
    "    f.writelines(str(seperator))\n",
    "    f.writelines(str(trainingLable))\n",
    "    f.writelines('\\n')\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileData = [fileData.rstrip(\"\\n\") for fileData in open(\"../TrainigDumpFile/wordtokanize.txt\", encoding='utf8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"../TrainigDumpFile/score1.txt\",\"w\",encoding='utf8') \n",
    "for row1 in fileData:\n",
    "    \n",
    "    sentence = row1.split(seperator)[0]  \n",
    "    positiveLexiconWordCount = row1.split(seperator)[1] \n",
    "    negtiveLexiconWordCount = row1.split(seperator)[2] \n",
    "    temppositiveByGram = row1.split(seperator)[3]\n",
    "    tempnegativeByGram = row1.split(seperator)[4]\n",
    "    trainingLable = row1.split(seperator)[7]\n",
    "    \n",
    "    _positiveByGram = str(temppositiveByGram)[1:-1]\n",
    "    _negativeByGram = str(tempnegativeByGram)[1:-1]    \n",
    "    positiveByGram = np.array(_positiveByGram)\n",
    "    \n",
    "    \n",
    "    count  = 0;\n",
    "    finalCount =0\n",
    "    if(positiveByGram != ''):\n",
    "        positiveByGram = ast.literal_eval(str(positiveByGram))\n",
    "        for i in positiveByGram:\n",
    "            count  = count +1  \n",
    "        finalCount =0.8\n",
    "        finalCount = finalCount +count\n",
    "        ##print(finalCount)      \n",
    "    f.writelines(sentence)\n",
    "    f.writelines(str(seperator))  \n",
    "    f.writelines(temppositiveByGram)\n",
    "    f.writelines(str(seperator)) \n",
    "    f.writelines(tempnegativeByGram)\n",
    "    f.writelines(str(seperator)) \n",
    "    f.writelines(str(finalCount))\n",
    "    f.writelines(str(seperator)) \n",
    "    f.writelines(trainingLable)\n",
    "    f.writelines(str(seperator)) \n",
    "    f.writelines(str(positiveLexiconWordCount))\n",
    "    f.writelines(str(seperator)) \n",
    "    f.writelines(str(negtiveLexiconWordCount))\n",
    "    f.writelines('\\n')\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoreData = [scoreData.rstrip(\"\\n\") for scoreData in open(\"../TrainigDumpFile/score1.txt\",encoding='utf8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"../TrainigDumpFile/score.txt\",\"w\",encoding='utf8') \n",
    "for row1 in scoreData:    \n",
    "    sentence = row1.split(seperator)[0]  \n",
    "    _positiveByGram = row1.split(seperator)[1]  \n",
    "    _negativeByGram1 = row1.split(seperator)[2]  \n",
    "    positiveScore = row1.split(seperator)[3] \n",
    "    trainingLable = row1.split(seperator)[4] \n",
    "    _negativeByGram = str(_negativeByGram1)[1:-1]    \n",
    "    negativeByGram = np.array(_negativeByGram) \n",
    "    positiveLexiconWordCount = row1.split(seperator)[5] \n",
    "    negtiveLexiconWordCount = row1.split(seperator)[6] \n",
    "     \n",
    "    count  = 0;\n",
    "    finalCount =0\n",
    "    if(negativeByGram != ''):\n",
    "        negativeByGram = ast.literal_eval(str(negativeByGram))\n",
    "        for i in negativeByGram:\n",
    "            count  = count +1  \n",
    "        finalCount =0.8\n",
    "        finalCount = finalCount +count\n",
    "        ##print(finalCount)      \n",
    "    f.writelines(sentence)\n",
    "    f.writelines(str(seperator)) \n",
    "    f.writelines(positiveScore)\n",
    "    f.writelines(str(seperator)) \n",
    "    f.writelines(str(finalCount))\n",
    "    f.writelines(str(seperator)) \n",
    "    f.writelines(_positiveByGram)\n",
    "    f.writelines(str(seperator)) \n",
    "    f.writelines(_negativeByGram1)\n",
    "    f.writelines(str(seperator)) \n",
    "    f.writelines(trainingLable)    \n",
    "    f.writelines(str(seperator)) \n",
    "    f.writelines(positiveLexiconWordCount)\n",
    "    f.writelines(str(seperator)) \n",
    "    f.writelines(negtiveLexiconWordCount)    \n",
    "    f.writelines('\\n')\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------END-------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"---------------END-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
